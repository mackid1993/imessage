# TPP: CloudKit Backfill Depth — Why Only Hundreds of Messages Per Chat

## Status
- **Phase**: Implementation complete, awaiting fresh deploy + verification
- **Last updated**: 2026-02-16
- **Branch**: `refactor` on `mackid1993/imessage-test`

## Goal
Determine why CloudKit backfill only retrieves a few hundred messages per chat when the full message history exists in iCloud (confirmed: messages sync correctly to other Apple devices). Fix the bridge to retrieve ALL messages.

## Research & Planning
- [x] Read CloudKit backfill pipeline (Go + Rust + protobuf)
- [x] Trace the full sync flow: Go pagination → Rust FFI → CloudKit API
- [x] Identify possible bottlenecks and silent data loss points
- [x] Add diagnostic logging to measure exactly where messages are lost
- [x] Compare `count_records()` server count vs. fetched count
- [ ] Check PCS key skip rate in production logs (needs fresh deploy)

### Required Reading
- `pkg/connector/sync_controller.go` — Go-side pagination and orchestration
- `pkg/connector/cloud_backfill_store.go` — Local DB storage
- `pkg/connector/connector.go` — Backfill config overrides in Start()
- `pkg/connector/identity_store.go` — Session state persistence + ListHandles()
- `pkg/rustpushgo/src/lib.rs` — FFI wrapper (cloud_sync_messages, cloud_sync_chats)
- `scripts/install-*.sh` — All 4 install scripts (backfill defaults + handle picker)

### Root Cause (SOLVED)
The CloudKit sync was actually fetching ALL messages correctly. The problem was
on the Matrix delivery side: mautrix defaults (`max_initial_messages=50`,
`batch_size=100`) caused hundreds of small backward `batch_send` requests that
created fragmented DAG branches. Clients could only paginate the last branch,
making it look like only ~1 week of messages existed. In reality, ALL 25K+
messages were bridged with valid Matrix event IDs.

## Tribal Knowledge
- `status == 3` in CloudKit `RetrieveChangesResponse` means "sync complete"
- `max_changes: None` lets server choose page size (~200 records typical)
- PCS = Protected Cloud Storage — Apple's per-record encryption in Manatee zones
- **DAG fragmentation**: Many small backward `batch_send` requests create parallel
  DAG branches that clients can't paginate. ONE large forward batch is correct.
- **bridgev2 startup order**: PreInit → LoadConfig → Init → Start → StartConnectors →
  connector.Start → StartLogins → RunBackfillQueue. Config overrides MUST go in
  `Start()` (not `Init()`) because config YAML loads between Init and Start.
- **bridgev2 backfill queue**: permanently deletes tasks when `portal.MXID == ""`.
  Only create tasks for portals with existing rooms; new portals get tasks
  automatically during ChatResync when `CanBackfill=true`.
- **Install scripts**: macOS uses `sed -i ''`, Linux uses `sed -i` (no empty arg).
  Beeper configs generated by `bbctl`, standalone by `$BINARY -c "$CONFIG" -e`.
- **list-handles CLI**: reads session.json directly (no logger!) to avoid stray
  output leaking into shell `$()` captures. Filter with `grep -E '^(tel:|mailto:)'`
  as defense-in-depth against any FFI/Rust log noise on stdout.

## Tasks

- [x] Task 1: Aggressive PCS key warm-up during init (Rust FFI)
  - TLK share refresh, 6 keychain sync retries, zone key pre-warming
  - Files: `pkg/rustpushgo/src/lib.rs`
  - Commit: `e8e5071`

- [x] Task 2: Add `count_records()` call at backfill start (Rust FFI + Go)
  - Log server-side totals during cloud client init
  - Files: `pkg/rustpushgo/src/lib.rs`
  - Commit: `3a154f5`

- [x] Task 3: Add per-page logging in Go pagination loops
  - Per-page: messages, status, done flag, has_token
  - Pagination stop reason: api_done vs token_unchanged
  - Files: `pkg/connector/sync_controller.go`
  - Commits: `e4c8c0c`

- [x] Task 4: Fix FFI panic pagination termination
  - Skip failed page + continue (consecutiveErrors counter, max 3)
  - Files: `pkg/connector/sync_controller.go`
  - Commit: `e8e5071`

- [x] Task 5: Version-gated full re-sync mechanism
  - `cloudSyncVersion` (now 4) clears tokens on upgrade
  - Don't save version on 0-record syncs (allow retry)
  - Files: `pkg/connector/sync_controller.go`, `cloud_backfill_store.go`
  - Commits: `e8e5071`, `e4c8c0c`

- [x] Task 6: Recovery for interrupted initial sync
  - Always restart attachment zone from scratch (in-memory attMap lost on crash)
  - Resume detection logging with per-zone token state
  - Files: `pkg/connector/sync_controller.go`, `cloud_backfill_store.go`
  - Commit: `e4f061e`

- [x] Task 7: Fix backward backfill race condition
  - `createPortalsFromCloudSync` was Upserting backfill tasks before rooms exist
  - bridgev2 permanently deletes tasks when `portal.MXID == ""`
  - Fix: Only Upsert tasks for portals with existing rooms; new portals
    get tasks created by bridgev2 during ChatResync (CanBackfill=true)
  - Files: `pkg/connector/sync_controller.go`
  - Commit: `9c4457d`

- [x] Task 8: Fix DAG fragmentation from too many backward batches
  - Root cause: mautrix defaults (max_initial_messages=50, batch_size=100)
    caused 251 backward batch_send requests creating unreachable DAG branches
  - Fix: High max_initial_messages (50000) delivers all history in one forward
    batch during room creation; batch_size=10000 reduces backward batches
  - Runtime override in connector.Start() + sed-patching in all 4 install scripts
  - Files: `pkg/connector/connector.go`, `scripts/install-*.sh`
  - Commit: `b4b1941`

- [x] Task 9: Interactive preferred handle picker in install scripts
  - Added `list-handles` CLI subcommand (reads session.json directly, no logger)
  - Added `ListHandles()` in identity_store.go
  - All 4 install scripts show numbered list of available handles
  - Defense-in-depth: grep filter `^(tel:|mailto:)` on binary output
  - Falls back to free-text if no session state exists
  - Writes choice to config.yaml (adds key if missing)
  - Files: `cmd/mautrix-imessage/main.go`, `pkg/connector/identity_store.go`,
    `scripts/install-*.sh`
  - Commit: `e484e4f`

- [x] Task 10: Fix stale group names and bridgebot messages in backfill
  - **Stale group names**: resolveGroupName cached stale CloudKit display_name
    into imGroupNames, which propagated to portal metadata via ExtraUpdates.
    On startup, stale metadata was pre-loaded back into imGroupNames, creating
    a vicious cycle. refreshGroupPortalNamesFromContacts and ChatResync would
    revert correct room names to stale values.
  - **Bridge bot text messages**: CloudKit messages with empty Sender field
    (system/notification records like renames, participant changes) were
    backfilled as regular text messages from the bridge bot (bridgev2 falls
    back to Bot when EventSender.Sender is empty).
  - Fixes:
    1. Stop resolveGroupName from caching CloudKit display_name into imGroupNames
    2. Stop pre-populating imGroupNames from portal metadata on startup
    3. Detect envelope name changes in makePortalKey and push room updates
    4. Skip setting name for existing portals in GetChatInfo (ChatResync)
    5. Update cloud_chat.display_name in handleRename
    6. Order cloud_chat display_name lookup by updated_ts DESC
    7. Skip backfill messages with empty sender (system records)
    8. ExcludeChangesFromTimeline on GetChatInfo as defense-in-depth
  - Files: `pkg/connector/client.go`, `pkg/connector/cloud_backfill_store.go`
  - Commit: `2bd3ee6`

- [ ] Task 11: Deploy and verify full backfill with fresh DB
  - Reset DB, run fresh sync with all fixes deployed
  - Verify: all messages visible in Beeper/Element (not just last week)
  - Verify: group names correct (not stale CloudKit names)
  - Verify: no bridge bot text messages in backfilled timelines
  - Check: forward backfill delivers full history in single batch
  - Check: preferred handle picker works cleanly (no stray log output)

- [ ] Task 12: Apply targeted fix if PCS skipping is still high
  - May need QueryRecordOperation with manual PCS decryption in lib.rs

## Implementation Log

### Session 1 (2026-02-15)
- Completed full code exploration of the CloudKit backfill pipeline
- Traced flow from Go orchestration → Rust FFI → CloudKit protobuf API
- Identified 6 potential bottlenecks (PCS skipping is top suspect)
- User confirmed: messages DO sync to other Apple devices (CloudKit HAS the data)
- Issue reported by multiple users (not account-specific)
- Created task breakdown for diagnostic-first approach

### Session 2 (2026-02-15)
- User wanted FIXES, not just logging ("fix it, we don't edit vendored rustpush")
- Implemented aggressive PCS key warm-up in lib.rs (TLK refresh, 6 retries, zone pre-warm)
- Fixed FFI panic: skip + continue instead of break (consecutiveErrors max 3)
- Added cloudSyncVersion mechanism for one-time full re-sync on upgrade
- Fixed backward backfill: Upsert instead of MarkNotDone, GetBackfillMaxBatchCount=-1
- Fixed COALESCE for NULL scan errors in cloud_message queries

### Session 3 (2026-02-16)
- Deployed v3: count_records diagnostic + version bump
- User logs showed backward backfill returning db_rows=0 for all portals
- Analysis: initial sync DID deliver thousands of messages (24K+ in one chat)
  but re-sync returned 0 because CloudKit considers records already "delivered"
- Fixed: don't save sync version on 0-record syncs (allow retry next restart)
- Added always-fire per-page logging and background CloudDiagFullCount diagnostic
- Implemented interrupted initial sync recovery (attachment zone token clearing)
- Fixed group name showing as bridge bot message: set ExcludeChangesFromTimeline

### Session 4 (2026-02-16)
- **Key discovery**: CloudKit sync was working all along! All 25,072 messages
  were bridged to Matrix with valid event IDs. The problem was DAG fragmentation:
  251 backward batch_send requests created parallel branches clients can't paginate.
- Fixed backward backfill race condition (tasks created before rooms exist)
- Fixed DAG fragmentation: runtime config override in connector.Start() sets
  max_initial=50000, batch_size=10000, max_batches=-1
- Updated all 4 install scripts with sed-patching for backfill defaults
  (new configs + upgrade existing low values)
- Squashed intermediate commits into `b4b1941` for clean history
- Added interactive preferred handle picker to install scripts:
  - `list-handles` CLI subcommand reads session.json directly (no logger)
  - Scripts show numbered list with `(current)` marker
  - grep filter defends against stray FFI output contaminating selection
  - Bug found: zerolog ConsoleWriter output leaked to stdout in `$()` captures
    even with `2>/dev/null`. Fixed by eliminating logger entirely (direct
    os.ReadFile + json.Unmarshal)
- Commit: `e484e4f`
- **Next**: deploy fresh, verify messages visible in clients, test handle picker

### Session 5 (2026-02-16)
- User reported two backfill issues: stale group names + bridge bot text messages
- **Stale group names root cause**: vicious cycle across restarts —
  1. resolveGroupName cached stale CloudKit display_name into imGroupNames
  2. GetChatInfo's ExtraUpdates persisted stale imGroupNames to portal metadata
  3. On restart, stale metadata pre-loaded back into imGroupNames
  4. refreshGroupPortalNamesFromContacts and ChatResync reverted correct names
  - First fix attempt (only-seed-if-empty in makePortalKey) made things WORSE
    by preventing real-time messages from correcting the stale cache
  - Correct fix: break the cycle at every source — don't cache CloudKit into
    imGroupNames, don't pre-load metadata into imGroupNames, don't set name
    for existing portals in GetChatInfo, detect envelope name changes
- **Bridge bot messages root cause**: CloudKit system/notification records
  (group renames, participant changes) have empty Sender field. bridgev2's
  GetIntentFor falls back to bridge bot when sender is unresolvable. These
  records were being backfilled as regular m.text messages from the bot.
  Fix: skip messages where sender is empty and IsFromMe is false.
- Also fixed: cloud_chat display_name query used LIMIT 1 with no ORDER BY,
  picking an arbitrary record. Now orders by updated_ts DESC.
- Also fixed: handleRename now updates cloud_chat.display_name so local
  CloudKit cache is corrected when renames arrive via APNs.
- Commit: `2bd3ee6`
- **Next**: deploy with fresh DB, verify group names + no bot messages
